{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install emoji"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXVsWdr2oWJv",
    "outputId": "5850c382-b770-41d4-ae1e-f4ce4507fe14"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (2.2.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fk6iUfUqlofa",
    "outputId": "7838cfc7-d8dd-45d0-b191-0f5d4bba7623"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V1BODPQ7iBDO"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('../Train/train_all_tasks.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    header = next(reader)\n",
    "    rows = [row for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "header"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orJW63ZUkk55",
    "outputId": "49b1fae8-e390-46e2-d961-b4bfbab2f5dd"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['rewire_id', 'text', 'label_sexist', 'label_category', 'label_vector']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rows[:2]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vJPPEhSi_iU",
    "outputId": "2cacf572-c5ef-4b00-86a9-32d91b015fb8"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['sexism2022_english-7358',\n",
       "  'Damn, this writing was pretty chaotic',\n",
       "  'not sexist',\n",
       "  'none',\n",
       "  'none'],\n",
       " ['sexism2022_english-2367',\n",
       "  'Yeah, and apparently a bunch of misogynistic virgins are the ones to turn a gay woman straight, lol',\n",
       "  'not sexist',\n",
       "  'none',\n",
       "  'none']]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import string\n",
    "from emoji import demojize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words_nltk = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "  def rep(m):\n",
    "    return ' '.join(re.split(r'(?=[A-Z])', m.group(1)))\n",
    "\n",
    "  text = re.sub(r'#(\\w+)', rep, text)\n",
    "\n",
    "  def fin_word(word):\n",
    "    if  word[0] == \"'\" and len(word) > 1 and word[1] not in [\"s\"]:\n",
    "      return \"'\" + lemmatizer.lemmatize(word[1:].lower()) + \"'\"\n",
    "    if word[0] == ':':\n",
    "      return lemmatizer.lemmatize(word[1:].lower())\n",
    "    return lemmatizer.lemmatize(word.lower())\n",
    "\n",
    "  text = ' '.join([fin_word(word)\n",
    "    for word in word_tokenize(demojize(text)) \n",
    "    if word.lower() not in [\"url\", \"user\"]\n",
    "    and not all([ch in string.punctuation for ch in word])\n",
    "    # and not word[0].isdigit() \n",
    "    and word != 'â€™'\n",
    "    and not word.lower() in stop_words_nltk])\n",
    "\n",
    "  return text\n",
    "\n",
    "# text = 'Thanks man, I know Iâ€™m a catch and Iâ€™m good looking just as we all are ðŸ’ª but I think I get too needy and am too available sometimes and these super attractive girls sniff it out'\n",
    "# text = 'Thought: God took on flesh by being born to a woman (became incarnate). Perhaps that is why the Left is so obsessed with their Pink PðŸ˜¸ðŸ˜¸ðŸ˜¸y hats and menstrual blood and wombs, etc etc. They are yearning for the mother who points the way toward her Son.'\n",
    "# text = \"Honestly, I think I could get an ugly and/or 40+ one tomorrow, they can be desperate like that. Especially some drunk slut -- easy. But that's cheating, she must be attractive to me as well to make it count.\"\n",
    "# text = \"BIAS ALERT: 'Furious' NYT writer pens attack on white women | [URL]\"\n",
    "# text =  \"That's very disappointing. Fire her. Now. #GabFam\"\n",
    "text = 'I said act like a whore. Also [URL] \"A woman who has many casual sexual encounters or relationships.\" \"A person who is regarded as willing to do anything to get a particular thing.\"'\n",
    "preprocess(text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "F4BCjOznkp7j",
    "outputId": "6f4c548c-33a9-4499-9922-9f85a353810c"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'said act like whore also woman many casual sexual encounter relationship person regarded willing anything get particular thing'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split data"
   ],
   "metadata": {
    "id": "2wlWKpeE95sh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# rows[0] - id\n",
    "# rows[1] - text\n",
    "# rows[2] - label\n",
    "texts = np.array([preprocess(line[1]) for line in rows])\n",
    "labels = np.array([1 if line[2] == \"sexist\" else 0 for line in rows], dtype='int')\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.20, random_state=42)"
   ],
   "metadata": {
    "id": "FtzuIXhTwk8W"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word_embedding(vectorizer):\n",
    "  if vectorizer == 'tfidf':\n",
    "    vectorizer = TfidfVectorizer(max_features = 1000)\n",
    "    train_data = vectorizer.fit_transform(train_texts)\n",
    "    test_data = vectorizer.transform(test_texts)\n",
    "  elif vectorizer == 'w2v':\n",
    "    def getVectorization(text, out_of_vocab):\n",
    "      vects = []\n",
    "      for word in text.split():\n",
    "          try:\n",
    "            vects.append(w2v.wv.get_vector(word))\n",
    "          except:\n",
    "            out_of_vocab = out_of_vocab + 1\n",
    "              # print(word)\n",
    "      return vects\n",
    "\n",
    "    # dataset = [preprocess(text).split() for text in np.array(list(train_texts) + brownData)]\n",
    "    dataset = [preprocess(text).split() for text in np.array(list(train_texts))]\n",
    "\n",
    "    w2v = Word2Vec(sentences=dataset, size=1000, window=5, min_count=1, workers=16)\n",
    "    w2v.save(\"word2vec.model\")\n",
    "\n",
    "    out_of_vocab = 0\n",
    "    train_data = [np.mean(np.array(getVectorization(preprocess(text), out_of_vocab)), axis=0) for text in train_texts]\n",
    "    test_data = [np.mean(np.array(getVectorization(preprocess(text), out_of_vocab)), axis=0) for text in test_texts]\n",
    "    print(out_of_vocab)\n",
    "\n",
    "  return train_data, test_data"
   ],
   "metadata": {
    "id": "99BhoOCB-dTX"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_tfidf, test_tfidf = word_embedding('tfidf')"
   ],
   "metadata": {
    "id": "F1XPdx6S-6B3"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_w2v, test_w2v = word_embedding('w2v')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnrjDMdq-8zV",
    "outputId": "8508b262-5844-4e5f-d359-788916d6c99a"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_data, test_data = train_tfidf.toarray(), test_tfidf.toarray()"
   ],
   "metadata": {
    "id": "MCuOZPiQ_4Uc"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(train_data, train_labels)\n",
    "print(np.mean(svc.predict(train_data) == train_labels))\n",
    "print(np.mean(svc.predict(test_data) == test_labels))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-wMpBYuCKaG",
    "outputId": "22ffd522-a077-49e5-fc5d-dad9253e5e09"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8963392857142857\n",
      "0.8210714285714286\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_data, test_data = np.array(train_w2v), np.array(test_w2v)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJxD1q9hBhvj",
    "outputId": "1259e63c-83d0-4c44-9d72-3a01185c4fb3"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(train_data, train_labels)\n",
    "print(np.mean(svc.predict(train_data) == train_labels))\n",
    "print(np.mean(svc.predict(test_data) == test_labels))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "gtBI3GtLCOGR",
    "outputId": "9efaed6a-8841-4917-899a-4dc95fabc955"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;31mTypeError\u001B[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-65d83cef1245>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0msvc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSVC\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0msvc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msvc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msvc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_data\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mtest_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"C\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                 \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"csr\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0maccept_large_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m             )\n\u001B[1;32m    198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    579\u001B[0m                 \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_y_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 581\u001B[0;31m                 \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_X_y\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    582\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    583\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m    974\u001B[0m         \u001B[0mensure_min_samples\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mensure_min_samples\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    975\u001B[0m         \u001B[0mensure_min_features\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mensure_min_features\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 976\u001B[0;31m         \u001B[0mestimator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mestimator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    977\u001B[0m     )\n\u001B[1;32m    978\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[1;32m    744\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcasting\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"unsafe\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 746\u001B[0;31m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    747\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    748\u001B[0m                 raise ValueError(\n",
      "\u001B[0;31mValueError\u001B[0m: setting an array element with a sequence."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ms_nKFEdE32n"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}