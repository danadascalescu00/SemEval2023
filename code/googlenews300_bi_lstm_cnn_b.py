# -*- coding: utf-8 -*-
"""GoogleNews300-Bi-LSTM-CNN_B.ipynb

Automatically generated by Colaboratory.
"""

"""# Task 10 SemEval2023 - Explainable Detection of Online Sexism (EDOS)

## Introduction
Sexism on social media is on the rise. It can negatively impact the mental health of some individuals and the lives of women who are specifically targeted. It may also cause harm by making online spaces inaccessible and unwelcoming, as well as by perpetuating socioeconomic divisions and inequities. Numerous organizations utilized automated technologies to search for and assess sexist content at scale; yet, these systems often provided only broad classifications of content without context or context-specific characteristics. By pointing out sexist content and explaining why it is offensive and unacceptable, we can help users and moderators better understand and accept the decisions made by automated technology. This data can also be used to determine how online movements affect people who have experienced trauma.

The task is described in detail on the [official website](https://codalab.lisn.upsaclay.fr/competitions/7124) for the competition.

According to the organizers the task contains three hierarchical subtasks:


*   **Task A:** Binary classification of whether or not a post is sexist.
*   **Task B:**  A four-class classification for sexist posts in which systems must predict one of four categories:  

  (1) *threats*, (2) *derogation*, (3) *animosity*, (4) *prejudiced discussions*.
*   **Task C:** For sexist posts, an eleven-class classification in which systems must predict one of eleven fine-grained vectors:

  (1.1) threats of harm
  (1.2) incitement and encouragement of harm

  (2.1) descriptive attacks
  (2.2) aggresive and emotive attacks
  (2.3) dehumanisation and overt sexual objectification

  (3.1) casual use of gendered slurs, profanities & insults
  (3.2) immutable gender stereotypes
  (3.3) backhanded gendered compliments
  (3.4) condescending explanations or unwelcome advice

  (4.1) supporting mistreatment of individual or unwelcome advice
  (4.2) supporting systemic discrimination against women



In this notebook, we explore strategies for analyzing the dataset, preprocessing the data, and extracting suitable features for ensemble approaches, and attention-based models.
"""

import os
import sys

import re
import csv
import string
import unicodedata

import numpy as np
import pandas as pd
from collections import Counter


import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import colors

import gensim
import gensim.parsing.preprocessing as gsp
from gensim import utils
# from gensim.summarization.textcleaner import split_sentences

import nltk
from nltk import FreqDist
from nltk.stem import WordNetLemmatizer

import warnings
warnings.filterwarnings("ignore")

nltk.download("punkt")
nltk.download("stopwords")
nltk.download('universal_tagset')
nltk.download("averaged_perceptron_tagger")
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 20}

matplotlib.rc('font', **font)

"""# Data Preprocessing



"""

raw_train_data = pd.read_csv('./data/train_all_tasks.csv')
raw_train_data.head(10)

raw_train_data = raw_train_data[raw_train_data['label_category'] != 'none']

raw_test_data_taskA = pd.read_csv('./data/dev_task_a_entries.csv')
raw_test_data_taskB = pd.read_csv('./data/dev_task_b_entries.csv')
# raw_test_data_taskC = pd.read_csv('/content/drive/MyDrive/NLP2/Test/dev_task_c_entries.csv')

"""## Data Cleaning and Standadrdization

Before proceeding with further analysis of the dataset, it is required to clean and preprocess the data:


1.   Since the 'text' column is the most important information in the dataset, we will delete any duplicate rows. Since there are no missing data, we shall skip the pre-processing stage in which missing information is removed.
2.   The text is decoded, and normalized, meaning that complicated symbols are turned into simple characters. Characters can be encoded in several ways, including "Latin," "ISO/IEC 8859-1," etc. Therefore, it is important to ensure comprehensive data in a standard encoding format for better analysis. We select UTF-8 encoding for this project because it is widely accepted and utilized.
3.   We substitute emoticons and emojis with their respective descriptors.

Beginning with a standard text cleaning procedure, we will then adapt the data preprocessing phase to our task and compare the results. A standard text cleaning process involves the following steps:


1.   Lowercase
2.   Removal of special characters (punctuations and numbers)
3.   Removal of extra whitespaces
4.   Removal of tags
5.   Removal of stop words
6.   Stemming/Lemmatizing (reducing inflexional forms to their root)
"""

# STEP 1 - skip for test data
raw_train_data.drop_duplicates(subset = ["text"], inplace = True)
print("Number of training samples: ", len(raw_train_data))

print("Number of test samples for task A: ", len(raw_test_data_taskA))
# print("Number of test samples for task B: ", len(raw_test_data_taskB))
# print("Number of test samples for task C: ", len(raw_test_data_taskC))

stopwords_nltk = stopwords.words('english')[:-36]
print(stopwords_nltk)
print(len(stopwords_nltk))

for word in ['but', 'until', 'against', 'up', 'down', 'off', 'on', 'no', 'nor', 'not', 'only']:
  stopwords_nltk.remove(word)
print(len(stopwords_nltk))

"""
    Emoticons data
"""

from emoji import demojize

EMOTICONS = [
    ("Laughing", r':[‑,-]?\){2,}'),
    ("Rolling_on_the_floor_laughing", r'\=\){2,}|\=\]'),
    ("Heart", r'<3'),
    ("Broken_heart", r'<\\3'),
    ('Very_happy', r':[‑,-]?D'),
    ('Happy_face_or_smiley', r'[:,8,=][‑,-,o,O]?\)|\(\^[v,u,o,O]\^\)|:[‑,-]?3'),
    ('Happy', r'=]'),
    ('Mischievous_smile', r':[‑,-]?>'),
    ('Sticking_tongue_out_playfulness_or_cheekiness', r':P|:[‑,-]P|;P|:b|:-b'),
    ('Kiss', r':[‑,-]?[\*,X,x]'),
    ('Joy', r' uwu | UwU '),
    ('Surprised_or_shock', r':[‑,-]?[o|O|0]|o_O|o_0'),
    ('Sad_frown_andry_or_pouting', r':[‑,-]?\('),
    ('Very_sad', r':[(]{2,}'),
    ('Crying', r':[‑,-]?\'\('),
    ('Straight_face_no_expression_dissaproval_or_not_funny', r':[‑,-]?\|'),
    ('Annoyed_or_hesitant', r'>?[:][\\|\/]|\=\/|=\\'),
    ('Angel_saint_or_innocent', r'[0,O,o]:[‑,-]?[\),3]'),
    ('Embarrassed_or_blushing', r':\$'),
    ('Sad_or_crying', r';_;|\(;_;\)|\(\'_\'\)|Q_Q|\(;_:\)|\(:_;\)'),
    ('Evil_or_devilish', r'[>|}|3]:[‑,-]?\)'),
    ('Laughing_big_grin_or_laugh_with_glasses', r'[:,8,X,=][-,‑]?[D,3]|B\^D'),
    ('Tears_of_happiness', r':[\',\`][‑,-]?\)'),
    ('Horror', r'D[-,‑]\''),
    ('Great_dismay', r'D[8,;,=]'),
    ('Tongue_in_cheek', r':[-,‑]J'),
    ('Yawn', r'8[‑,-]0|>:O'),
    ('Sadness', r'D:'),
    ('Disgust', r'D:<'),
    ('Cool', r'\|;[‑,-]\)'),
    ('Drunk_or_confused', r'%[-,‑]?\)'),
    ('Sealed_lips_or_wearing_braces_or_tongue_tied', r':[-,‑]?[x,#,&]'),
    ('Skeptical_annoyed_undecided_uneasy_or_hesitant', r':[-,‑]?[.,/]|:[L,S]|=[/,L]'),
    ('Scepticism_disbelief_or_disapproval', r'\',:-\||\',:[-,-]'),
    ('Party_all_night', r'#‑\)'),
    ('Headphones_listening_to_music', r'\(\(d\[-_-\]b\)\)'),
    ('Bored', r'\|‑O'),
    ('Dump', r'<:‑\|'),
    ('Being_sick', r':-?#{2,3}..'),
    ('Amazed', r'\(\*_\*\)|\(\+_\+\)|\(\@_\@\)'),
    ('Confusion', r'\(\?_\?\)|\(\・\・?'),
    ('Wink_or_smirk', r';[-,‑]?[\),D,\]]|\*[-,‑]?\)|;\^\)|:‑,|;3'),
    ('Exciting', r'\\\(\^o\^\)\/|\\\(\^o\^\)\／|ヽ\(\^o\^\)丿|\(\*^0^\*\)|＼\(-o-\)／|＼\(~o~\)\／'),
    ('Giggling_with_hand_covering_mouth', r'\^m\^'),
    ('Joyful', r'\(\^_\^\)/|\(\^[O,o]\^\)／|\(°o°\)'),
    ('Tired', r'\(=_=\)'),
    ('Shame', r'\(-_-\)|\(一_一\)'),
    ('Surprised', r'\(o\.o\)'),
    ('Sleeping', r'\(-_-\)zzz'),
    # ('Kowtow_as_a_sign_of_respect_or_dogeza_for_apology', r'\(__\)|_\(\._\.\)_|<\(_ _\)>|m\(_ _\)m|m\(__\)m||_\(_\^_\)_'),
    ('Troubled', r'\(>_<\)>?'),
    ('Nervous__Embarrassed_Troubled_Shy_Sweat_drop', r'\(-_-;\)|\(\^_\^;\)|\(-_-;\)|\(~_~;\)|\(・.・;\)|\(・_・;\)'),
    ('Wink', r'\(\^_-\)'),
    ('Normal_laugh', r'>\^_\^<|<\^!\^>|\(\^\.\^\)|\(\^J\^\)|\(\*\^[_,.]\^\*\)|\(\^<\^\)|\(\^\.\^\)|\(#\^\.\^#\)'),
    ('STH_ELSE', r'.')
]

emoticons_tokens = '|'.join('(?P<%s>%s)' % emoticon for emoticon in EMOTICONS)

def replace_emoticons(text):
    new_text = ""
    for match in re.finditer(emoticons_tokens, text):
        emoticon_name = match.lastgroup
        emoticon = match.group(emoticon_name)
        if emoticon_name == 'STH_ELSE':
            new_text += emoticon
        else:
            new_text += emoticon_name
    return new_text

lemmatizer = WordNetLemmatizer()

cleaning_filters = [
    gsp.strip_tags,
    gsp.strip_punctuation,
    gsp.strip_multiple_whitespaces,
    gsp.strip_numeric,
    # gsp.remove_stopwords,
    gsp.strip_short,
]

def clean_and_standardize_text(text):
  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('UTF-8', 'ignore')
  text = replace_emoticons(text).lower()
  for filter in cleaning_filters:
    text = filter(text)
  lemmas = [lemmatizer.lemmatize(t) for t in word_tokenize(text) if t not in stopwords_nltk]
  text = ' '.join(lemmas)
  return text

clean_train_data = raw_train_data['text'].apply(clean_and_standardize_text)
clean_test_data = raw_test_data_taskB['text'].apply(clean_and_standardize_text)

clean_train_data

"""# Dataset preparation"""

import tensorflow as tf
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard
from tensorflow import keras
from keras import layers
from keras import backend as K

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

words_vocabulary = set()
for text in clean_train_data:
  words_vocabulary.update(set(text.split()))
print(len(words_vocabulary))

SEED = 1042

np.random.seed(SEED)
keras.utils.set_random_seed(SEED)

WORDS_MAX_FEATURES = len(words_vocabulary)
WORDS_EMBEDDING_SIZE = 300
BATCH_SIZE = 64
EPOCHS = 100

"""### Validation Split"""

X_train, X_valid, y_train, y_valid = train_test_split(clean_train_data, raw_train_data['label_category'], test_size=0.25,
                                                      random_state=SEED, shuffle=True, stratify=raw_train_data['label_category'])

label_encoder = OneHotEncoder()

y_train = label_encoder.fit_transform(y_train.to_numpy().reshape(-1, 1)).toarray()
y_valid = label_encoder.transform(y_valid.to_numpy().reshape(-1, 1)).toarray()

# Creating TF Datasets for faster prefetching and parallelization
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))
test_dataset = tf.data.Dataset.from_tensor_slices((clean_test_data))

MAX_WORDS_SEQ_LENGTH = max(X_train.apply(lambda x: len(x.split()))) + 1

words_vectorizer_layer = layers.TextVectorization(WORDS_MAX_FEATURES, output_sequence_length=MAX_WORDS_SEQ_LENGTH)
# Adapting the dataset
words_vectorizer_layer.adapt(train_dataset.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE))

"""### Google Word2Vec"""

word2vec = gensim.models.KeyedVectors.load_word2vec_format('./pretrained_embeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)

print("Found %s word vectors." % len(word2vec.vectors))

voc = words_vectorizer_layer.get_vocabulary()
word_index = dict(zip(voc, range(len(voc))))

num_tokens = words_vectorizer_layer.vocabulary_size() + 2
hits = 0
misses = 0

# Prepare embedding matrix
embedding_matrix = np.zeros((num_tokens, WORDS_EMBEDDING_SIZE))
for word, i in word_index.items():
    if word in word2vec:
      embedding_vector = word2vec.word_vec(word)
      if embedding_vector is not None:
          # Words not found in embedding index will be all-zeros.
          # This includes the representation for "padding" and "OOV"
          embedding_matrix[i] = embedding_vector
          hits += 1
    else:
        misses += 1
print("Converted %d words (%d misses)" % (hits, misses))

"""### Metrics"""

@tf.function
def macro_balanced_soft_f1(y, y_hat):
    """Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).
    Use probability values instead of binary predictions.
    This version uses the computation of soft-F1 for both positive and negative class for each label.
    
    Args:
        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)
        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)
        
    Returns:
        cost (scalar Tensor): value of the cost function for the batch
    """
    y = tf.cast(y, tf.float32)
    y_hat = tf.cast(y_hat, tf.float32)
    tp = tf.reduce_sum(y_hat * y, axis=0)
    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)
    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)
    tn = tf.reduce_sum((1 - y_hat) * (1 - y), axis=0)
    soft_f1_class1 = 2*tp / (2*tp + fn + fp + 1e-16)
    soft_f1_class0 = 2*tn / (2*tn + fn + fp + 1e-16)
    cost_class1 = 1 - soft_f1_class1 # reduce 1 - soft-f1_class1 in order to increase soft-f1 on class 1
    cost_class0 = 1 - soft_f1_class0 # reduce 1 - soft-f1_class0 in order to increase soft-f1 on class 0
    cost = 0.5 * (cost_class1 + cost_class0) # take into account both class 1 and class 0
    macro_cost = tf.reduce_mean(cost) # average on all labels
    return macro_cost

@tf.function
def f1_macro_score(y, y_hat, thresh=0.5):
    """Compute the macro F1-score on a batch of observations (average F1 across labels)
    
    Args:
        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)
        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)
        thresh: probability value above which we predict positive
        
    Returns:
        macro_f1 (scalar Tensor): value of macro F1 for the batch
    """
    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)
    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)
    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)
    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)
    f1 = 2*tp / (2*tp + fn + fp + 1e-16)
    macro_f1 = tf.reduce_mean(f1)
    return macro_f1

def make_submissions(predictions, ids):
  preds = np.zeros_like(predictions)
  preds[np.arange(len(predictions)), predictions.argmax(1)] = 1
  preds = label_encoder.inverse_transform(preds)
  df = pd.DataFrame(zip(ids, preds[:, 0]), columns=['rewire_id', 'label_pred'])
  return df

"""### Callbacks"""

early_stop = EarlyStopping(
    monitor='val_loss',
    min_delta=0.0001,
    patience=10,
    verbose=1,
    restore_best_weights=True,
)

reduce_lron_plateau = ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.1,
    patience=3,
    verbose=1,
    mode="auto",
    min_delta=0.0001,
    cooldown=1,
    min_lr=0.000001,
)

"""### GoogleNews300-BiLSTM-CNN-words"""

train_dataset_words = train_dataset.batch(BATCH_SIZE)
valid_dataset_words = valid_dataset.batch(BATCH_SIZE)
test_dataset = test_dataset.batch(BATCH_SIZE)

def build_bilstm_cnn_words_model(lstm_units=64, dense_units=256, dense_dropout=0.4):
  input = tf.keras.Input(shape=(1, ), dtype=tf.string, name='text') # input
  x = words_vectorizer_layer(input) # vectorize text 
  x = layers.Embedding(embedding_matrix.shape[0], WORDS_EMBEDDING_SIZE, weights=[embedding_matrix],
                            input_length=MAX_WORDS_SEQ_LENGTH,
                            trainable=False)(x) # get embeddings 
  x = layers.Dropout(0.3)(x)

  conv_bigrams = layers.Conv1D(100, 2, padding='same', activation="relu", 
                               kernel_initializer='he_normal')(x)
  conv_trigrams = layers.Conv1D(100, 3, padding='same', activation="relu", 
                                kernel_initializer='he_normal')(x)

  max_pool_bigrams = layers.MaxPool1D(pool_size=2)(conv_bigrams)
  max_pool_trigrams = layers.MaxPool1D(pool_size=2)(conv_trigrams)

  x = layers.concatenate([max_pool_bigrams, max_pool_trigrams])

  x = layers.Bidirectional(layers.LSTM(lstm_units))(x)
  x = layers.Dropout(0.3)(x)

  x = layers.Dense(dense_units, activation='relu', kernel_initializer='he_normal')(x)
  x = layers.Dropout(dense_dropout)(x)
  x = layers.Dense(dense_units // 2, activation='relu', kernel_initializer='he_normal')(x)
  x = layers.Dropout(dense_dropout)(x)
  outputs = layers.Dense(4, activation='softmax')(x)

  # Instantiate an end-to-end model 
  model = keras.Model(inputs=input, outputs=outputs)

  model.compile(keras.optimizers.Adam(learning_rate=0.0001), loss=macro_balanced_soft_f1,
                  metrics=[f1_macro_score])

  return model

bilstm_cnn_words_model = build_bilstm_cnn_words_model(lstm_units=128, dense_units=512)
bilstm_cnn_words_model.summary()

# keras.utils.plot_model(cnn_words_model, './cnn_words_model.png', show_shapes=True)

history = bilstm_cnn_words_model.fit(train_dataset_words, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True,
                              validation_data=valid_dataset_words, callbacks=[early_stop, reduce_lron_plateau,
                                                                   TensorBoard('./tensorboard/tb_logs_bilstm_cnn_google_news_300_B')])

dev_test_preds = bilstm_cnn_words_model.predict(test_dataset)

bilstm_cnn_words_dev_B_submissions = make_submissions(dev_test_preds, raw_test_data_taskA['rewire_id'])
bilstm_cnn_words_dev_B_submissions.to_csv('./submissions/bilstm_cnn_google_news_dev_B.csv', index=False)
bilstm_cnn_words_dev_B_submissions.head()

"""### GoogleNews300-BiLSTM"""

def build_bilstm_model(lstm_units=64, dense_units=256, dense_dropout=0.4):
  input = tf.keras.Input(shape=(1, ), dtype=tf.string, name='text') # input
  x = words_vectorizer_layer(input) # vectorize text 
  x = layers.Embedding(embedding_matrix.shape[0], WORDS_EMBEDDING_SIZE, weights=[embedding_matrix],
                            input_length=MAX_WORDS_SEQ_LENGTH,
                            trainable=False)(x) # get embeddingss

  x = layers.Dropout(0.3)(x)

  conv_bigrams = layers.Conv1D(64, 2, padding='same', activation="relu", 
                               kernel_initializer='he_normal')(x)
  conv_trigrams = layers.Conv1D(32, 3, padding='same', activation="relu", 
                                kernel_initializer='he_normal')(x)

  max_pool_bigrams = layers.MaxPool1D(pool_size=2)(conv_bigrams)
  max_pool_trigrams = layers.MaxPool1D(pool_size=2)(conv_trigrams)

  x = layers.concatenate([max_pool_bigrams, max_pool_trigrams])

  x = layers.Bidirectional(layers.LSTM(lstm_units))(x)
  x = layers.Dropout(0.3)(x)

  x = layers.Dense(dense_units, activation='relu', kernel_initializer='he_normal')(x)
  x = layers.Dropout(dense_dropout)(x)
  x = layers.Dense(dense_units // 2, activation='relu', kernel_initializer='he_normal')(x)
  x = layers.Dropout(dense_dropout)(x)

  outputs = layers.Dense(5, activation='softmax')(x)

  # Instantiate an end-to-end model 
  model = keras.Model(inputs=input, outputs=outputs)

  model.compile(keras.optimizers.Adam(learning_rate=0.0001), loss=macro_balanced_soft_f1,
                  metrics=[f1_macro_score])

  return model

bilstm_model = build_bilstm_model(lstm_units=128, dense_units=128)
bilstm_model.summary()

history = bilstm_model.fit(train_dataset_words, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True,
                              validation_data=valid_dataset_words, callbacks=[early_stop, reduce_lron_plateau,
                                                                   TensorBoard('./tensorboard/tb_logs_bilstm_google_news_300_B')])

dev_test_preds = bilstm_model.predict(test_dataset)

bilstm_cnn_words_dev_B_submissions = make_submissions(dev_test_preds, raw_test_data_taskA['rewire_id'])
bilstm_cnn_words_dev_B_submissions.to_csv('./submissions/bilstm_google_news_dev_B.csv', index=False)
bilstm_cnn_words_dev_B_submissions.head()